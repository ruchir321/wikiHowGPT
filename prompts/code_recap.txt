I forgot what my code from 1 month back is for.
Help me writing documentation worthy comments.
Do not change any logic.

In the end give me a summary of how i can improve the code.
import os
import dotenv

from langchain_redis import RedisConfig, RedisVectorStore

#prompt template
# from langchain import hub
from langchain_core.prompts import  ChatPromptTemplate

# semantic cache
from redisvl.extensions.llmcache import SemanticCache

# LLM memory
from redisvl.extensions.session_manager import SemanticSessionManager
import uuid

from langchain_ollama import ChatOllama, OllamaEmbeddings
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

import streamlit as st

dotenv.load_dotenv()

# eval
os.environ["LANGSMITH_API_KEY"] = dotenv.dotenv_values()["LANGSMITH_API_KEY"]
os.environ["LANGSMITH_TRACING"] = dotenv.dotenv_values()["LANGSMITH_TRACING"]
os.environ["LANGSMITH_PROJECT"] = dotenv.dotenv_values()["LANGSMITH_PROJECT"]

REDIS_URL = dotenv.dotenv_values()["redis_vector_db"]
REDIS_SEMANTIC_CACHE_URL = dotenv.dotenv_values()["redis_semantic_cache"]
REDIS_LLM_MEMORY_URL = dotenv.dotenv_values()["redis_llm_memory"]

PROMPT_TEMPLATE = "rlm/rag-prompt"
# EMBEDDING_MODEL = "nomic-embed-text"
EMBEDDING_MODEL = "llama3.2" # TODO: change to nomic
LLM_MODEL = "gemma3:4b"

import torch

torch.classes.__path__ = [os.path.join(torch.__path__[0], torch.classes.__file__)] 

# or simply:
torch.classes.__path__ = []

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

def create_prompt_template():
    prompt_template = ChatPromptTemplate(
        messages=[
            ("system", "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nContext: {context}"),
            ("human", "{question}")
        ]
    )

    return prompt_template

@st.cache_resource
def load_resources():
    # query embedding model
    embedding_model = OllamaEmbeddings(model=EMBEDDING_MODEL)
    
    # vector db connection
    config = RedisConfig(
        index_name="article",
        redis_url=REDIS_URL
    )

    vector_store = RedisVectorStore(embeddings=embedding_model, config=config)

    # generative LLM
    llm = ChatOllama(
        model=LLM_MODEL
    )
    
    # chat prompt template
    # prompt = hub.pull(PROMPT_TEMPLATE)
    prompt = create_prompt_template()

    # doc retrieval tool
    retriever = vector_store.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 4}
    )

    # chain
    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    return rag_chain

@st.cache_data
def get_model_output(query, session_id):

    chat_history = llmmemory.get_recent(top_k=5, session_tag=session_id, as_text=True)

    print(chat_history)
    # formatted_history = "\n".join(
    #     [f"{msg["role"]}: {msg["content"]}" for msg in chat_history]
    # )

    # full_history = f"{formatted_history}\nUser: {query}\nAssistant:"

    results = llmcache.check(
        prompt=query,
        num_results=1,
        distance_threshold=0.1,
        return_fields=['response']
    )

    if results:
        print("Cache hit!")
        return results[0]["response"]

    else:
        print("Cache miss")
        response = rag_chain.invoke(query)
        llmcache.store(
            prompt=query,
            response=response
        )
        print("Cache set!")

    # add user query to chat history
    llmmemory.add_message(
        session_tag=session_id,
        role="user",
        content=query
    )

    # add llm response to chat history
    llmmemory.add_message(
        session_tag=session_id,
        role="assistant",
        content=response
    )

    return response


# streamlit app
st.set_page_config(
    page_title="wikiHowGPT",
    page_icon=":bulb:"
)

# Initialize resources once
rag_chain = load_resources()

llmmemory = SemanticSessionManager(
    name="llm_memory",
    redis_url=REDIS_LLM_MEMORY_URL,
    # vectorizer=EMBEDDING_MODEL # use default
)

llmcache = SemanticCache(
            name="wikiGPTcache",
            # vectorizer=OllamaEmbeddings(model=EMBEDDING_MODEL), # the default is sentence-transformers/all-mpnet-base-v2
            distance_threshold=0.1,
            redis_url=REDIS_SEMANTIC_CACHE_URL,
            ttl=1024
            )

st.header("Know the How")
form_input = st.text_input("Enter query")
submit = st.button("Generate")

# session id
session_id = st.session_state.get('session_id', str(uuid.uuid4())) # check if session_id exists, else set new uuid
st.session_state['session_id'] = session_id

if submit:
    st.write(get_model_output(form_input, session_id))
